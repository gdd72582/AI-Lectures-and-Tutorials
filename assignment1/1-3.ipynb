{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 인공지능 개념"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "### 1-1. 인공지능, 기계학습, 그리고 딥러닝 간의 상관관계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![1.1](./figs/1.1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### AI(Artificial Intelligence): \n",
        "- 인간의 지능이 필요하다고 여겨지는 작업을 컴퓨터 시스템이 수행하도록 만든 컴퓨터 시스템\n",
        "- 컴퓨터가 인간의 인지/학습/추론/문제해결 과정을 모방한다.\n",
        "    - 예시: 과일분리 AI 기반 알고리즘, 컨베이어 벨트에 사과가 있으면 사과임을 알리고 사과 과일 트레이로 옮긴다.\n",
        "\n",
        "##### ML(Machine Learning): \n",
        "- 컴퓨터가 데이터를 통해 학습하고 예측 및 최적화할 수 있도록 하는 기술\n",
        "- 고전적인 머신러닝의 특징은 사람이 알려준 패턴을 식별하고 학습한다.\n",
        "    - 예시: 각 과일이 어떻게 생겼는지에 대한 정의, 각 과일을 짓는 특징, 크기, 색상, 모양 등과 같은 특징을 추출하여 알고리즘을 훈련시켜 과일을 분류한다.\n",
        "\n",
        "##### DL(Deep Learning): \n",
        "- 대규모의 비구조화된 데이터로부터 특징을 자동으로 학습하여 복잡한 문제해결에 사용되는 기술.\n",
        "- Deep Learning에서는 여러층의 Neural Network를 사용하며 Deep은 층 깊이를 의미한다.\n",
        "- Neural Network는 뇌의 뉴런 신호를 주고 받는 방식을 모방한 형태를 말한다.\n",
        "    - 예시: 훨씬 더 많은 과일을 분류하며, 전에 본 적 없는 과일에 대한 특징을 제공하지 않아도 분류한다.\n",
        "- 머신러닝과 딥러닝의 차이점:\n",
        "    | 구분           | 머신러닝                          | 딥러닝                                          |\n",
        "    |----------------|-----------------------------------|------------------------------------------------|\n",
        "    | 데이터 양       | 상대적으로 적은 데이터              | 대규모 데이터 필요 (Big Data)                   |\n",
        "    | 데이터 종류     | 구조화된 데이터 (표 형태, 수치 등)   | 비구조화된 데이터 (이미지, 음성, 텍스트 등)       |\n",
        "    | 특성 추출       | 사람이 직접 Feature를 설계          | Feature Extraction이 자동화                    |\n",
        "    | 모델 복잡도     | 비교적 단순한 모델                   | 다층 신경망(Deep Neural Network) 구조 사용       |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1-2. 딥러닝이 2012년에 성공적인 결과를 만들 수 있었던 배경"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![1.2](./figs/1.2.jpeg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 결과 개요\n",
        "2012년, ImageNet 대회에서 Alexnet은 기존보다 10%p 이상 향상된 성과를 얻었다.\n",
        "\n",
        "### 성장 배경\n",
        "\n",
        "1. **시대적 배경**\n",
        "- 대규모 이미지 데이터셋 (ImageNet)\n",
        "- GPU 기반 학습 확산\n",
        "- CNN(Convolutional Neural Network) 도입 본격화\n",
        "\n",
        "2. **기술적 배경**\n",
        "- ReLU: 활성화 함수로, 학습 속도 개선 및 기울기 소실 문제 완화\n",
        "- Dropout: 과적합 방지 기술\n",
        "- Data Augmentation: 이미지 회전, 이동 등으로 데이터를 늘려 과적합 방지\n",
        "\n",
        "### 이후 영향\n",
        "- 모델 구조 심화: 더 깊은 네트워크 설계 가능\n",
        "- GPU 시장 급성장 (AI 연산 수요 증가)\n",
        "- Transfer Learning 표준화: 사전 학습 모델 활용 확대\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 딥러닝 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 딥러닝에서 학습을 한다는 것은 어떤 의미인가? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![2.1](./figs/2.1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "딥러닝에서 학습이란, 모델의 가중치, 편향을 조정하여 손실함수 값을 줄여가는 과정이다.\n",
        "\n",
        "구체적으로 다음의 단계로 이루어진다.\n",
        "1. 손실함수 정의\n",
        "2. 매개변수 초기화\n",
        "3. gradient 계산\n",
        "4. 매개변수 업데이트\n",
        "5. 위 과정을 반복(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 반복 학습 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![2.2](./figs/2.2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1 Feed Forward\n",
        "![feedforward](https://s3.amazonaws.com/ebooks.syncfusion.com/LiveReadOnlineFiles/Neural_Networks_Using_C_Sharp_Succinctly/Images/neural-network-architecture.png)\n",
        "- 입력 데이터를 받아 신경망을 통과시키며 출력(prediction)을 계산한다.\n",
        "- 각 레이어는 이전 출력에 가중치와 편향을 적용하고, 비선형 활성화 함수를 통해 출력을 만든다.\n",
        "\n",
        "2 손실 계산 및 역전파 (Loss Computation & Backpropagation)\n",
        "![gradient](https://s3.amazonaws.com/ebooks.syncfusion.com/LiveReadOnlineFiles/Neural_Networks_Using_C_Sharp_Succinctly/Images/the-back-propagation-mechanism.png)\n",
        "- 출력과 실제 정답 간의 차이를 손실 함수로 계산한다.\n",
        "- 역전파는 이 손실 값에 대해\n",
        "    - 각 매개변수가 얼마나 손실에 영향을 미치는지, 즉 gradient를 계산하는 과정이다.\n",
        "    - 계산 방법은 chain rule을 기반으로 한다.\n",
        "출력층부터 입력층 방향으로 거슬러 올라가며\n",
        "중간 계산값을 이용해 각 레이어의 가중치에 대한 경사를 효율적으로 계산한다.\n",
        "- 이 과정은 손실의 변화가 가중치에 미치는 영향을 역으로 추적하는 것이다.\n",
        "\n",
        "3 매개변수 업데이트 (Parameter Update)\n",
        "- 계산된 경사를 이용해 가중치를 손실이 줄어드는 방향으로 조정한다.\n",
        "- 기본적으로 Gradient Descent을 사용하며, SGD가 계산 효율을 높이고 최적화를 돕는다.\n",
        "\n",
        "4 반복 (Epoch)\n",
        "- 위 1~3단계 과정을 전체 훈련 데이터에 대해 반복 수행한다.\n",
        "- 데이터셋을 한 번 모두 학습한 과정을 epoch라고 부른다.\n",
        "- 일반적으로:\n",
        "    - 손실 함수가 더 이상 감소하지 않거나,\n",
        "    - 별도의 검증 데이터셋에서 성능이 최고점에 도달했을 때,\n",
        "학습을 중단한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Backpropagation and Chain Rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Gradient가 backpropagation에서 어떻게 사용되나?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![3.1](./figs/3.1.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 그라디언트\n",
        "    - 그라디언트 자체는 얼마나 변하는지/변동성/방향성을 띈 값\n",
        "\n",
        "- 그라디언트 행렬\n",
        "    - 행렬A를 b로 편미분하면 b가 행렬A에게 미치는 영향력\n",
        "    - 이때, 이 '영향력'이란 것의 표현이 gradient matrix로 사용될 수 있다.\n",
        "\n",
        "- Backpropagation\n",
        "    - 거꾸로 편미분 해가는 과정 (chain rule)\n",
        "\n",
        "- 그라디언트행렬이 Backpropagation에서 어떻게 사용되는지 설명\n",
        "    - 각 layer를 손실함수로 편미분하면 \"손실함수가 각 layer에게 미치는 영향력\"\n",
        "    - 미치는 영향력이 gradient matrix로 표현됌\n",
        "\n",
        "- Optimization\n",
        "    - 각 레이어에 gradient matrix를 참고해 파라미터를 업데이트한다\n",
        "    - 업데이트하는 방향이 정해져있어요 -> Graident \"Descent\"\n",
        "        - 손실함수가 layer에게 미치는 영향력이 작아지게 업데이트\n",
        "        - 손실함수는 예측값이랑 정답값이 차이가나면 커지죠\n",
        "        - 예측값이랑 정답값의 차이가 적게나도록 파라미터를 업데이트해간다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Chain rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "→ Chain rule은 함수의 합성에 대한 미분 규칙이다. 딥러닝에서는 여러 레이어가 연결되어 있으므로, 각 레이어의 gradient를 계산할 때 chain rule을 사용하여 연쇄적으로 미분한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 특정 레이어의 downstream gradient는 어떻게 계산되는 지 아는 대로 쓰세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![3.3](./figs/3.3.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "특정레이어의 downstream gradient는 해당 레이어 이후의 gradient와 그 레이어의 출력에 대한 미분값을 곱해 계산한다. 즉, 이전 레이어에서 전달받은 gradient 와 현재 레이어의 local gradient의 곱으로 구한다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
