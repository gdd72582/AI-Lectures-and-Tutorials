{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbnH6KDLoNUH"
      },
      "source": [
        "# Activation Function: ReLU vs Sigmoid\n",
        "\n",
        "In this notebook, we compare the performance of ReLU and Sigmoid activation functions as the depth of a convolutional neural network increases. This is based on the known fact that sigmoid suffers from gradient vanishing, while ReLU maintains better gradient flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conditions:**\n",
        "- Compare two networks that are identical except for the activation function: sigmoid vs relu.\n",
        "- Vary the number of blocks from 1 to 8 and measure performance.\n",
        "- Each block consists of: Convolution (or FC) → BatchNorm → Activation.\n",
        "\n",
        "**Expected Outcome:**\n",
        "- As depth increases, networks with ReLU should show clearly better performance than those with Sigmoid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Theoretical Background\n",
        "**Sigmoid Function:**\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "- Output range: (0, 1)\n",
        "- Suffering from vanishing gradients for large positive/negative inputs\n",
        "\n",
        "**ReLU Function:**\n",
        "$$ReLU(x) = \\max(0, x)$$\n",
        "\n",
        "- Output range: [0, ∞)\n",
        "- Helps prevent vanishing gradient\n",
        "\n",
        "**Gradient Vanishing Example:** In a deep network, sigmoid derivatives shrink gradients over layers, stalling learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experimental Setup\n",
        "- **Dataset**: CIFAR-10\n",
        "- **Model**: Custom CNN with 1 to 8 stacked blocks\n",
        "- **Block Architecture**: Convolution → BatchNorm → Activation\n",
        "- **Activation**: ReLU vs Sigmoid\n",
        "- **Evaluation**: Top-1 Accuracy\n",
        "- **Runs**: Each experiment repeated 3 times for averaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H05prhTToNUJ",
        "outputId": "d08db0f6-a9b8-427e-c845-68956967eb8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENkfCDtRoNUK"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "UPba514_oNUK"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    'num_blocks_list': [1, 2, 4, 6, 8],\n",
        "    'num_epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'learning_rate': 0.001,\n",
        "    'seed': 42\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ApE72ThoNUL"
      },
      "source": [
        "## Data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "QNkJuK5CoNUL"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                          download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=hyperparameters['batch_size'],\n",
        "                                             shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                         download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=hyperparameters['batch_size'],\n",
        "                                            shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djRdi1rJoNUL"
      },
      "source": [
        "## BasicBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3RqdpcV6oNUL"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.activation(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja2IQsfWoNUL"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NYtI44y5oNUM"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, num_blocks, activation='relu'):\n",
        "        super(Network, self).__init__()\n",
        "        self.activation = activation\n",
        "\n",
        "        # initial convolution\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        if activation == 'relu':\n",
        "            self.activation1 = nn.ReLU()\n",
        "        else:\n",
        "            self.activation1 = nn.Sigmoid()\n",
        "\n",
        "        # basic block\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for _ in range(num_blocks):\n",
        "            self.blocks.append(BasicBlock(64, 64, activation))\n",
        "\n",
        "        # last layer\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation1(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61InVo3doNUM"
      },
      "source": [
        "## Train, Evaluate\n",
        "\n",
        "### Gradient Magnitude Tracking\n",
        "\n",
        "In addition to standard training metrics (loss and accuracy),  \n",
        "we monitor the average magnitude of gradients across all trainable parameters for each epoch.\n",
        "\n",
        "This measurement serves as an indirect indicator of training dynamics—particularly the effectiveness of gradient propagation through the network.  \n",
        "Such trends are informative when comparing activation functions, as vanishing gradients can severely impair learning in deep architectures.\n",
        "\n",
        "Note: The gradient statistics are not required by the problem setup but are included to provide further insight into model behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "xBlA9h3NoNUM"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    grad_list = []  # list to save gradient\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Accumulate per-batch average gradient magnitude\n",
        "        # This serves to quantify how gradient flow evolves during training\n",
        "        # and indirectly reflects the degree of vanishing or stability across layers.\n",
        "        batch_gradients = []\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                batch_gradients.append(param.grad.abs().mean().item())\n",
        "        \n",
        "        # Store the average gradient magnitude across all trainable parameters\n",
        "        if batch_gradients:\n",
        "            grad_list.append(sum(batch_gradients) / len(batch_gradients))\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # Compute epoch-level average gradient magnitude for monitoring\n",
        "    # Gradient magnitude trends can help interpret training stability\n",
        "    # especially in the context of activation functions.\n",
        "    if grad_list:\n",
        "        epoch_grad_mean = sum(grad_list) / len(grad_list)\n",
        "        print(f\"Epoch Gradient Mean: {epoch_grad_mean:.6f}\")\n",
        "\n",
        "    return running_loss / len(train_loader), 100. * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "LGlItm1soNUM"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / len(test_loader), 100. * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuzWZ6XPoNUM"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOwfq4muoNUM",
        "outputId": "2bf207cd-7ece-49a2-b7a2-0e4186e8141f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing with 1 blocks\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Gradient Mean: 0.006450\n",
            "ReLU - Train Acc: 36.53%, Test Acc: 39.87%\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Gradient Mean: 0.009449\n",
            "ReLU - Train Acc: 45.41%, Test Acc: 39.71%\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Gradient Mean: 0.011012\n",
            "ReLU - Train Acc: 48.66%, Test Acc: 48.01%\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Gradient Mean: 0.012106\n",
            "ReLU - Train Acc: 50.59%, Test Acc: 47.12%\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Gradient Mean: 0.012917\n",
            "ReLU - Train Acc: 52.40%, Test Acc: 50.24%\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Gradient Mean: 0.013662\n",
            "ReLU - Train Acc: 53.52%, Test Acc: 47.47%\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Gradient Mean: 0.014247\n",
            "ReLU - Train Acc: 54.35%, Test Acc: 53.35%\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Gradient Mean: 0.014282\n",
            "ReLU - Train Acc: 55.40%, Test Acc: 53.92%\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Gradient Mean: 0.014698\n",
            "ReLU - Train Acc: 56.19%, Test Acc: 54.99%\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch Gradient Mean: 0.015103\n",
            "ReLU - Train Acc: 56.84%, Test Acc: 54.76%\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Gradient Mean: 0.004192\n",
            "Sigmoid - Train Acc: 22.83%, Test Acc: 27.17%\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Gradient Mean: 0.006369\n",
            "Sigmoid - Train Acc: 29.50%, Test Acc: 30.93%\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Gradient Mean: 0.007654\n",
            "Sigmoid - Train Acc: 33.18%, Test Acc: 32.88%\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Gradient Mean: 0.008304\n",
            "Sigmoid - Train Acc: 35.35%, Test Acc: 35.32%\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Gradient Mean: 0.008900\n",
            "Sigmoid - Train Acc: 37.11%, Test Acc: 38.51%\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Gradient Mean: 0.009211\n",
            "Sigmoid - Train Acc: 38.32%, Test Acc: 35.70%\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Gradient Mean: 0.010082\n",
            "Sigmoid - Train Acc: 39.94%, Test Acc: 39.86%\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Gradient Mean: 0.010900\n",
            "Sigmoid - Train Acc: 41.92%, Test Acc: 42.23%\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Gradient Mean: 0.011851\n",
            "Sigmoid - Train Acc: 43.43%, Test Acc: 41.96%\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch Gradient Mean: 0.012027\n",
            "Sigmoid - Train Acc: 44.83%, Test Acc: 44.59%\n",
            "\n",
            "Testing with 2 blocks\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Gradient Mean: 0.005560\n",
            "ReLU - Train Acc: 40.80%, Test Acc: 46.55%\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Gradient Mean: 0.008041\n",
            "ReLU - Train Acc: 53.08%, Test Acc: 51.63%\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Gradient Mean: 0.009048\n",
            "ReLU - Train Acc: 57.82%, Test Acc: 56.05%\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Gradient Mean: 0.009774\n",
            "ReLU - Train Acc: 60.80%, Test Acc: 48.54%\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Gradient Mean: 0.010277\n",
            "ReLU - Train Acc: 63.09%, Test Acc: 53.74%\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Gradient Mean: 0.010671\n",
            "ReLU - Train Acc: 64.60%, Test Acc: 61.31%\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Gradient Mean: 0.011129\n",
            "ReLU - Train Acc: 66.04%, Test Acc: 63.27%\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Gradient Mean: 0.011257\n",
            "ReLU - Train Acc: 67.18%, Test Acc: 51.03%\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Gradient Mean: 0.011610\n",
            "ReLU - Train Acc: 68.19%, Test Acc: 66.11%\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch Gradient Mean: 0.011769\n",
            "ReLU - Train Acc: 69.18%, Test Acc: 64.09%\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Gradient Mean: 0.003432\n",
            "Sigmoid - Train Acc: 23.85%, Test Acc: 27.99%\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Gradient Mean: 0.004966\n",
            "Sigmoid - Train Acc: 30.56%, Test Acc: 32.03%\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Gradient Mean: 0.005580\n",
            "Sigmoid - Train Acc: 35.01%, Test Acc: 35.61%\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Gradient Mean: 0.006038\n",
            "Sigmoid - Train Acc: 38.70%, Test Acc: 35.75%\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Gradient Mean: 0.006581\n",
            "Sigmoid - Train Acc: 42.13%, Test Acc: 39.32%\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Gradient Mean: 0.006864\n",
            "Sigmoid - Train Acc: 45.09%, Test Acc: 35.98%\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Gradient Mean: 0.007188\n",
            "Sigmoid - Train Acc: 47.87%, Test Acc: 42.32%\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Gradient Mean: 0.007099\n",
            "Sigmoid - Train Acc: 49.53%, Test Acc: 39.03%\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Gradient Mean: 0.006952\n",
            "Sigmoid - Train Acc: 51.36%, Test Acc: 37.91%\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch Gradient Mean: 0.007053\n",
            "Sigmoid - Train Acc: 52.94%, Test Acc: 40.78%\n",
            "\n",
            "Testing with 4 blocks\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Gradient Mean: 0.004006\n",
            "ReLU - Train Acc: 44.59%, Test Acc: 50.23%\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Gradient Mean: 0.005380\n",
            "ReLU - Train Acc: 59.22%, Test Acc: 61.68%\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Gradient Mean: 0.006035\n",
            "ReLU - Train Acc: 65.59%, Test Acc: 65.33%\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Gradient Mean: 0.006372\n",
            "ReLU - Train Acc: 69.08%, Test Acc: 68.62%\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Gradient Mean: 0.006588\n",
            "ReLU - Train Acc: 71.99%, Test Acc: 67.52%\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Gradient Mean: 0.006829\n",
            "ReLU - Train Acc: 74.08%, Test Acc: 67.94%\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Gradient Mean: 0.007013\n",
            "ReLU - Train Acc: 75.89%, Test Acc: 71.51%\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Gradient Mean: 0.007120\n",
            "ReLU - Train Acc: 77.41%, Test Acc: 62.55%\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Gradient Mean: 0.007305\n",
            "ReLU - Train Acc: 78.78%, Test Acc: 72.05%\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch Gradient Mean: 0.007462\n",
            "ReLU - Train Acc: 79.94%, Test Acc: 73.63%\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Gradient Mean: 0.002413\n",
            "Sigmoid - Train Acc: 24.86%, Test Acc: 25.93%\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Gradient Mean: 0.003261\n",
            "Sigmoid - Train Acc: 33.32%, Test Acc: 27.98%\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Gradient Mean: 0.003432\n",
            "Sigmoid - Train Acc: 38.30%, Test Acc: 33.37%\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Gradient Mean: 0.003555\n",
            "Sigmoid - Train Acc: 42.98%, Test Acc: 30.38%\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Gradient Mean: 0.003798\n",
            "Sigmoid - Train Acc: 47.29%, Test Acc: 40.11%\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Gradient Mean: 0.004040\n",
            "Sigmoid - Train Acc: 50.57%, Test Acc: 27.66%\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Gradient Mean: 0.004152\n",
            "Sigmoid - Train Acc: 52.89%, Test Acc: 43.65%\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Gradient Mean: 0.004251\n",
            "Sigmoid - Train Acc: 55.21%, Test Acc: 32.03%\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Gradient Mean: 0.004372\n",
            "Sigmoid - Train Acc: 56.85%, Test Acc: 28.30%\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch Gradient Mean: 0.004475\n",
            "Sigmoid - Train Acc: 58.74%, Test Acc: 15.86%\n",
            "\n",
            "Testing with 6 blocks\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Gradient Mean: 0.003330\n",
            "ReLU - Train Acc: 46.71%, Test Acc: 49.80%\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Gradient Mean: 0.004366\n",
            "ReLU - Train Acc: 62.49%, Test Acc: 60.56%\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Gradient Mean: 0.004618\n",
            "ReLU - Train Acc: 68.57%, Test Acc: 61.91%\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Gradient Mean: 0.004833\n",
            "ReLU - Train Acc: 72.53%, Test Acc: 64.12%\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Gradient Mean: 0.004947\n",
            "ReLU - Train Acc: 75.83%, Test Acc: 72.41%\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Gradient Mean: 0.005096\n",
            "ReLU - Train Acc: 78.27%, Test Acc: 74.77%\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Gradient Mean: 0.005233\n",
            "ReLU - Train Acc: 79.98%, Test Acc: 72.82%\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Gradient Mean: 0.005303\n",
            "ReLU - Train Acc: 81.75%, Test Acc: 74.40%\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Gradient Mean: 0.005391\n",
            "ReLU - Train Acc: 83.00%, Test Acc: 78.84%\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch Gradient Mean: 0.005526\n",
            "ReLU - Train Acc: 84.11%, Test Acc: 75.39%\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Gradient Mean: 0.001852\n",
            "Sigmoid - Train Acc: 25.29%, Test Acc: 28.78%\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Gradient Mean: 0.002440\n",
            "Sigmoid - Train Acc: 33.87%, Test Acc: 30.91%\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Gradient Mean: 0.002629\n",
            "Sigmoid - Train Acc: 40.19%, Test Acc: 26.18%\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Gradient Mean: 0.002946\n",
            "Sigmoid - Train Acc: 44.50%, Test Acc: 38.04%\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Gradient Mean: 0.003142\n",
            "Sigmoid - Train Acc: 48.59%, Test Acc: 25.54%\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Gradient Mean: 0.003311\n",
            "Sigmoid - Train Acc: 50.81%, Test Acc: 24.06%\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Gradient Mean: 0.003314\n",
            "Sigmoid - Train Acc: 53.12%, Test Acc: 19.96%\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Gradient Mean: 0.003480\n",
            "Sigmoid - Train Acc: 54.97%, Test Acc: 20.52%\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Gradient Mean: 0.003511\n",
            "Sigmoid - Train Acc: 56.88%, Test Acc: 21.68%\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch Gradient Mean: 0.003515\n",
            "Sigmoid - Train Acc: 59.03%, Test Acc: 42.43%\n",
            "\n",
            "Testing with 8 blocks\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Gradient Mean: 0.002594\n",
            "ReLU - Train Acc: 44.78%, Test Acc: 47.98%\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Gradient Mean: 0.003491\n",
            "ReLU - Train Acc: 62.62%, Test Acc: 62.52%\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Gradient Mean: 0.003657\n",
            "ReLU - Train Acc: 69.23%, Test Acc: 65.75%\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Gradient Mean: 0.003803\n",
            "ReLU - Train Acc: 73.15%, Test Acc: 69.42%\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Gradient Mean: 0.003954\n",
            "ReLU - Train Acc: 76.30%, Test Acc: 72.36%\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Gradient Mean: 0.004080\n",
            "ReLU - Train Acc: 78.80%, Test Acc: 73.06%\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Gradient Mean: 0.004199\n",
            "ReLU - Train Acc: 80.88%, Test Acc: 74.87%\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Gradient Mean: 0.004243\n",
            "ReLU - Train Acc: 82.73%, Test Acc: 74.60%\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Gradient Mean: 0.004290\n",
            "ReLU - Train Acc: 84.01%, Test Acc: 79.76%\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch Gradient Mean: 0.004444\n",
            "ReLU - Train Acc: 85.23%, Test Acc: 80.07%\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Gradient Mean: 0.001538\n",
            "Sigmoid - Train Acc: 25.59%, Test Acc: 29.00%\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Gradient Mean: 0.002000\n",
            "Sigmoid - Train Acc: 33.99%, Test Acc: 28.26%\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Gradient Mean: 0.002128\n",
            "Sigmoid - Train Acc: 39.68%, Test Acc: 27.10%\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Gradient Mean: 0.002307\n",
            "Sigmoid - Train Acc: 45.25%, Test Acc: 23.06%\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Gradient Mean: 0.002469\n",
            "Sigmoid - Train Acc: 48.96%, Test Acc: 12.90%\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Gradient Mean: 0.002581\n",
            "Sigmoid - Train Acc: 52.09%, Test Acc: 32.97%\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Gradient Mean: 0.002697\n",
            "Sigmoid - Train Acc: 54.13%, Test Acc: 31.65%\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Gradient Mean: 0.002715\n",
            "Sigmoid - Train Acc: 56.83%, Test Acc: 20.51%\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Gradient Mean: 0.002774\n",
            "Sigmoid - Train Acc: 59.34%, Test Acc: 29.29%\n",
            "\n",
            "Epoch 10/10\n",
            "Epoch Gradient Mean: 0.002824\n",
            "Sigmoid - Train Acc: 61.61%, Test Acc: 25.71%\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# save results\n",
        "results = {\n",
        "    'relu': {'train_acc': [], 'test_acc': []},\n",
        "    'sigmoid': {'train_acc': [], 'test_acc': []}\n",
        "}\n",
        "\n",
        "for num_blocks in hyperparameters['num_blocks_list']:\n",
        "    print(f\"\\nTesting with {num_blocks} blocks\")\n",
        "\n",
        "    # ReLU \n",
        "    relu_model = Network(num_blocks, activation='relu').to(device)\n",
        "    optimizer = optim.Adam(relu_model.parameters(), lr=hyperparameters['learning_rate'])\n",
        "\n",
        "    best_test_acc = 0\n",
        "    for epoch in range(hyperparameters['num_epochs']):\n",
        "        print(f\"\\nEpoch {epoch+1}/{hyperparameters['num_epochs']}\")\n",
        "        train_loss, train_acc = train_model(relu_model, train_loader, criterion, optimizer, device)\n",
        "        test_loss, test_acc = evaluate_model(relu_model, test_loader, criterion, device)\n",
        "\n",
        "        print(f'ReLU - Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
        "        best_test_acc = max(best_test_acc, test_acc)\n",
        "\n",
        "    results['relu']['test_acc'].append(best_test_acc)\n",
        "\n",
        "    # Sigmoid \n",
        "    sigmoid_model = Network(num_blocks, activation='sigmoid').to(device)\n",
        "    optimizer = optim.Adam(sigmoid_model.parameters(), lr=hyperparameters['learning_rate'])\n",
        "\n",
        "    best_test_acc = 0\n",
        "    for epoch in range(hyperparameters['num_epochs']):\n",
        "        print(f\"\\nEpoch {epoch+1}/{hyperparameters['num_epochs']}\")\n",
        "        train_loss, train_acc = train_model(sigmoid_model, train_loader, criterion, optimizer, device)\n",
        "        test_loss, test_acc = evaluate_model(sigmoid_model, test_loader, criterion, device)\n",
        "\n",
        "        print(f'Sigmoid - Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
        "        best_test_acc = max(best_test_acc, test_acc)\n",
        "\n",
        "    results['sigmoid']['test_acc'].append(best_test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PunnuZH-wnyX",
        "outputId": "7351d42f-4987-44e7-bc20-93b97d84e126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Summary: \n",
            "Blocks     ReLU (%)        Sigmoid (%)    \n",
            "1          54.99           44.59          \n",
            "2          66.11           42.32          \n",
            "4          73.63           43.65          \n",
            "6          78.84           42.43          \n",
            "8          80.07           32.97          \n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Summary: \")\n",
        "print(\"{:<10} {:<15} {:<15}\".format(\"Blocks\", \"ReLU (%)\", \"Sigmoid (%)\"))\n",
        "for i, num_blocks in enumerate(hyperparameters['num_blocks_list']):\n",
        "    relu_acc = results['relu']['test_acc'][i]\n",
        "    sigmoid_acc = results['sigmoid']['test_acc'][i]\n",
        "    print(\"{:<10} {:<15.2f} {:<15.2f}\".format(num_blocks, relu_acc, sigmoid_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "|  | **ReLU** | **ReLU** | **Sigmoid** | **Sigmoid** |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| **Block** | **Test Accuracy (%)** | **Gradient Avg** | **Test Accuracy (%)** | **Gradient Avg** |\n",
        "| 1 | 54.99 | 0.0124 | **44.59** | 0.0090 |\n",
        "| 2 | 66.11 | 0.0099 | 42.32 | 0.0062 |\n",
        "| 4 | 73.63 | 0.0064 | 43.65 | 0.0038 |\n",
        "| 6 | 78.84 | 0.0049 | 42.43 | 0.0030 |\n",
        "| 8 | **80.07** | 0.0039 | 32.97 | 0.0024 |\n",
        "\n",
        "\n",
        "Test Accuracy Comparison by Number of Blocks\n",
        "- ReLU: 54.99 → 80.07 (+25.08%)\n",
        "- Sigmoid: 44.59 → 32.97 (-11.62%)\n",
        "\n",
        "Gradient Magnitude Reduction\n",
        "- ReLU: 0.0124 → 0.0039 (-68.5%)\n",
        "- Sigmoid: ~0.0090 → 0.0024 (-73.3%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis\n",
        "\n",
        "The empirical results indicate that both ReLU and Sigmoid activation functions exhibit a reduction in average gradient magnitude as network depth increases. Notably, the extent of gradient attenuation in Sigmoid was approximately 4.8 percentage points greater than in ReLU.\n",
        "\n",
        "Despite the shared trend of diminishing gradients, the performance trajectories of the two activations diverged markedly. ReLU demonstrated consistent improvements in classification accuracy, whereas Sigmoid suffered substantial degradation in deeper architectures.\n",
        "\n",
        "This disparity can be attributed to the fundamental characteristics of their respective gradient behaviors. ReLU, while yielding zero gradient for non-positive inputs, preserves a constant gradient in the positive regime, thereby enabling effective error propagation and stable learning in deep networks. In contrast, Sigmoid saturates for large-magnitude inputs, driving gradients toward zero and exacerbating the vanishing gradient problem, which ultimately impedes convergence during training.\n",
        "\n",
        "Collectively, the findings underscore the unsuitability of Sigmoid for deep neural networks, as evidenced by its severely diminished representational and learning capacity in such contexts—an outcome consistent with theoretical expectations."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
