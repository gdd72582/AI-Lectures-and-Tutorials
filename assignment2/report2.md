# Report2 - CNN 아키텍처 실습

## 1. AlexNet에서 data augmentation을 사용하면 성능이 오르는 지 확인

| **실험 조건** | **실험 1** | **실험 2** | **실험 3** | **실험 4** | **실험 5** | **평균 정확도** |
| --- | --- | --- | --- | --- | --- | --- |
| baseline | 0.8256 | 0.8245 | 0.8339 | 0.8348 | 0.8322 | 0.8291 |
| with_data_augmentation | 0.8639 | 0.8692 | 0.8626 | 0.8597 | 0.8617 | 0.8634 |
| add_augmentation | 0.8587 | 0.8720 | 0.8726 | 0.8701 | 0.8706 | 0.8688 |

## **분석**

본 실험은 동일한 AlexNet 모델과 학습 설정 하에, 랜덤 시드(seed)만 다르게 설정하여 총 5회씩 실험을 반복 수행하였다. 

**1. baseline**

데이터 증강 없이 수행된 실험으로, 평균 정확도는 약 82.9%로 측정되었다. 실험 간 성능 편차는 약 ±0.5% 수준으로 비교적 안정적인 수렴 경향을 보였다.

**2. with_data_augmentation**

기본적인 수평 뒤집기 및 랜덤 크롭을 포함한 데이터 증강을 적용한 조건으로, 평균 정확도는 약 86.3%로 baseline 대비 약 3.4%p 향상되었다. 또한, 실험 간 결과의 일관성도 높았으며, 이는 증강된 데이터가 일반화 성능에 긍정적인 영향을 미쳤음을 보여준다.

**3. add_augmentation**

기본 증강 외에 추가적으로 Color Jitter, Rotation, RandomAffine 등의 기법이 포함된 조건으로, 평균 정확도는 약 86.9%로 가장 높았다. 단일 실험에서 대부분 87% 근처의 성능을 보였다.

## 결론

모든 조건에서 시드에 따라 일부 편차는 있었지만, 데이터 증강을 적용한 조건에서 평균 성능이 명확하게 우수하였다.

---

## 2. VGG에서는 3x3 convolution만을 사용하는데 7x7 convolution을 사용하는 경우와 어떤 차이가 있는지 실험적으로 분석

(파라미터의 수, 연산시간, 분류 성능에 대한 비교분석 필요)

## **실험 결과**

| **모델** | **실험 1** | **실험 2** | **실험 3** | **실험 4** | **실험 5** | **평균 정확도** | **시간 (평균, sec/epoch)** | **파라미터 수** |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| VGG 3x3 | 0.8750 | 0.8686 | 0.8720 | 0.8638 | 0.8663 | 0.8691 | 22.8 | 33,638,218 |
| VGG 7x7 | 0.8302 | 0.8200 | 0.8265 | 0.8266 | 0.8202 | 0.8247 | 29.8 | 99,018,058 |

## **분석**

1. **성능 차이**
    
    VGG 3x3 모델이 평균 정확도 86.9%로 VGG 7x7의 82.5%보다 약 4.4%p 높은 성능을 기록하였다. 이는 작은 커널을 반복적으로 쌓을 경우 더 많은 비선형성을 도입할 수 있어 표현력이 높아지기 때문이다.
    
2. **파라미터 수 및 연산 시간 차이:** 
    - **파라미터 수:** VGG 7x7 모델은 VGG 3x3 모델보다 약 **2.9배 많은 파라미터 수**를 가진다.
    - **연산 시간 차이:** Epoch당 학습 시간 또한 VGG 7x7 모델이 VGG 3x3 모델보다 약 **23% 더 소요**되었다.
    
    연산 효율 측면에서도 작은 커널을 여러 개 조합하는 방식이 유리하다.
    

## **결론**

본 실험을 통해, VGG 구조에서 3x3 Convolution을 사용하는 것이 **성능**, **연산 효율**, **파라미터 효율성** 측면에서 7x7 Convolution보다 우수함을 확인하였다.

- 3x3 커널은 더 깊은 네트워크 구조를 가능하게 하고, 효율적인 학습과 높은 일반화 성능을 제공한다.
- 7x7 커널은 이론상 넓은 수용영역을 가지지만, 실제로는 학습 안정성과 성능 면에서 불리할 수 있다.

따라서, VGG 계열 네트워크에서는 작은 커널을 반복적으로 사용하는 전략이 더 바람직한 선택임을 실험적으로 검증하였다.

---

## 3. Inception module에서 1x1 convolution을 사용해서 채널을 줄인 후 3x3, 5x5 convolution을 사용하는데 1x1 convolution을 사용하지 않으면 사용했을 때와 어떤 차이가 있는지 실험적으로 분석

(파라미터의 수, 연산시간, 분류 성능에 대한 비교분석 필요)

| **Inception module** | **1** | **2** | **3** | **4** | **5** | **평균 정확도** | **평균 학습 시간 (s)** | **파라미터 수** |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **with 1×1 conv** | 0.7415 | 0.8076 | 0.8157 | 0.8148 | 0.8147 | **0.79886** | **1923.64** | **320,042** |
| **without 1×1 conv** | 0.7635 | 0.8302 | 0.8243 | 0.8301 | 0.8222 | **0.81406** | **1169.27** | **661,386** |

## **분석**

1. **성능 차이**
    
    1x1 conv를 사용하지 않은 모델이 **평균 정확도 0.8140**으로, 사용한 모델(0.7988)보다 약 **1.5%p 더 높은 성능**을 보였다.
    
    - 이는 1x1 conv를 사용하지 않아 채널을 줄이지 않음으로써, 모델이 **정보 손실 없이 더 복잡하고 다양한 패턴을 학습**할 수 있었던 것으로 보인다.
    - 다만, 파라미터 수가 많다는 것은 **overfitting의 잠재적 위험**이 있기 때문에 추가적인 실험해보는  것이 좋을 것 같다.
2. **파라미터 수 및 연산 시간 차이:** 
    - **파라미터 수:** 1x1 conv를 사용한 모델의 파라미터 수는 약 32만 개로, 미사용한 모델(약 66만 개)의 **절반 수준**이었다.
    - **연산 시간 차이:** 파라미터 수가 적음에도 불구하고, 1x1 conv를 포함한 모델의 학습 시간은 평균 **619초**로, 미사용한 모델(169초)보다 약 **3.6배 더 오래** 걸렸다.
        - 이는 1x1 conv를 통한 채널 축소 모듈이 GPU 병렬 처리 효율을 저해하고, 작은 연산들이 많아지면서 발생하는 Kernel Launch Overhead가 누적된 결과로 분석된다.
        - 또한, 여러 브랜치에서 독립적으로 작용하는 1x1 conv와 이후 결과 병합 과정에서 메모리 접근 패턴이 비효율적이 되어 캐시 효율성이 떨어진 것도 원인일 수 있다.
3. **기대와 다른 결과에 대한 원인 분석**

일반적으로 1x1 conv는 연산량 감소와 정보 결합에 이점이 있다고 알려져 있지만, 본 실험에서는 예상과 다른 결과가 나타났다. 이는 다음과 같은 복합적인 요인들 때문이다.

1. **과도한 채널 압축에 따른 정보 손실**: 1x1 conv가 채널을 과도하게 줄였을 경우, 중요한 특성 정보까지 손실되어 모델의 표현력 저하 및 성능 하락을 초래할 수 있다.
2. **하드웨어 및 프레임워크 최적화 한계**: PyTorch와 같은 딥러닝 프레임워크에서 작은 다중 Conv 연산들의 병렬 처리 효율이 이론만큼 높지 않을 수 있으며, 이는 GPU에서의 실제 연산 시간 증가로 이어진다.
3. **하이퍼파라미터 설정의 비최적성**: 두 모델은 구조적으로 다르지만 동일한 하이퍼파라미터(학습률, 옵티마이저 등)를 적용했을 가능성이 높다. 파라미터 수가 적은 모델은 더 섬세한 학습 조건이 요구되는데, 이에 대한 최적화가 부족했을 수 있다.

## **결론**

본 실험을 통해 1x1 Convolution이 파라미터 수를 줄이는 데는 효과적이나, 실제 학습 시간 단축이나 정확도 향상으로 직결되지 않을 수 있음을 확인했다. 특히 1x1 conv를 포함한 Inception 구조가 오히려 연산 효율 저하와 성능 감소를 초래한 것은, 단순한 모델 경량화가 항상 최적의 결과를 보장하지 않음을 보여준다.

반면, 1x1 conv를 제거한 구조는 더 많은 파라미터를 가짐에도 불구하고 **학습 시간과 정확도 측면에서 일관된 우수한 성능**을 보였다. 이는 딥러닝 모델 설계 시, 이론적인 연산량 감소 효과뿐만 아니라 **GPU 아키텍처의 특성, 메모리 접근 패턴, 그리고 특정 데이터셋의 복잡도** 등 다양한 실제적 요소를 함께 고려하여 **표현력과 계산 효율성 간의 균형**을 찾는 것이 중요함을 시사한다.

## 4. VGG와 Inception에서 layer의 수를 각각 50개를 쌓게 되면 원래 네트워크와 비교해서 성능과 Gradient가 레이어 별로 어떻게 변화하는 지 확인.

(수업시간에 두 네트워크의 경우 이렇게 layer의 수를 무작정 늘리면 성능이 오히려 좋아지지 않는다고 했습니다. 이러한 결과가 실제로 발생하는 지 확인해 보세요. Gradient를 backpropagation 연산 과정을 학생들이 custom하게 코딩 할 수 있도록 변형을 해야지 값을 확인해 볼 수 있을 겁니다.)

| **모델 구조** | **정확도 (Accuracy)** |
| --- | --- |
| VGG-16 | 0.8434 |
| VGG-50 | 0.3876 |
| Inception-v1 | 0.7586 |
| Inception-50 | 0.7609 |

### 1. 실험 결과 요약 및 Gradient 흐름 분석

첨부된 Gradient Flow 그래프는 Epoch에 따른 세 가지 대표 레이어의 평균 Gradient Norm 변화를 보여준다.

### 1.1. VGG-50

- **성능**: 정확도 38.7%로 VGG-16 (84.3%) 대비 **급격한 성능 하락**을 보였다.
- **Epoch 단위 Gradient**: 그래프에서 주황색 선(`features.0.weight`)과 녹색 선(`features.78.bias`)은 **학습 내내 Gradient Norm이 거의 0에 수렴하는 평평한 양상**을 명확히 보여준다. 이는 **초기 레이어 및 깊은 레이어로 Gradient가 제대로 전달되지 않는 심각한 Vanishing Gradient 현상**이 발생했음을 의미한다. 반면, 파란색 선(`classifier.6.bias`)은 비교적 높은 Gradient를 유지하며 불규칙하게 변화하는데, 이는 네트워크 전체의 학습이 불균형하게 진행되고 **상위 분류기 계층만 주로 학습**되었음을 시사한다. 이 현상이 낮은 정확도로 직결되었다.
- **Layer 별 Gradient**: `features.0.weight` (가장 초기 Conv 레이어) 및 `features.78.bias` (매우 깊은 특징 추출 레이어 또는 마지막 Conv 블록 내의 Bias)의 Gradient가 거의 0에 수렴하는 것은, 네트워크 전체가 학습되지 못하고 **상위 일부 계층만 학습되거나 과적합**되는 문제점을 명확히 보여준다.

### 1.2. VGG-16

- **성능**: 84.3%의 높은 정확도를 달성했다.
- **Epoch 단위 Gradient**: 그래프에서 주황색 선(`features.5.weight`, 중간 깊이 레이어로 추정)은 초기 높은 값에서 시작하여 **점진적으로 감소하며 비교적 안정적인 Gradient 흐름**을 보인다. 파란색 선(`classifier.6.bias`) 또한 학습 초기에 높은 Gradient를 보이다 점차 수렴하는 양상을 나타낸다. 녹색 선(`features.24.bias`, 깊은 특징 추출 레이어로 추정)은 0에 가깝지만 VGG-50보다는 약간 높은 값을 유지한다. 전반적으로 **더 큰 Gradient Norm을 유지하며 여러 레이어에서 안정적인 수준으로 흐름**이 관측되었다.
- **Layer 별 Gradient**: 중간 깊이의 레이어(`features.5.weight`)에서도 의미 있는 Gradient가 관측되었으나, 최상위 레이어(`classifier.6.bias`) 대비 Gradient 값은 학습이 진행됨에 따라 감소한다. 이는 깊이에 따른 Gradient 감소 경향이 VGG 구조의 특징임을 보여준다.

### 1.3. Inception-50

- **성능**: Inception-v1 (75.8%)과 유사한 76.0%의 안정적인 정확도를 유지했다.
- **Epoch 단위 Gradient**: 그래프에서 파란색 선(`conv2d.1.conv.weight`, 비교적 초기 레이어)은 초기 높은 수준에서 시작되나, Epoch이 진행됨에 따라 **점차 완만하게 감소**한다. 주황색 선(`inception_layer.4.branch.3.1.conv.bias`, Inception 모듈 내부의 깊은 레이어)과 녹색 선(`linear.bias`, 최종 분류기 레이어)은 Gradient Norm이 0에 가깝지만 **극히 낮은 수준을 꾸준히 유지**하며 크게 변화하지 않는다. 이 그래프는 50층 규모의 깊은 구조임에도 불구하고 **Gradient 소실 문제가 거의 발생하지 않고 고르게 흐름**을 시각적으로 확인시켜 준다.
- **Layer 별 Gradient**: 네트워크의 깊은 특징 추출 레이어들에서도 Gradient가 효과적으로 전달되어 **모든 레이어가 학습에 고르게 기여**했다. 최종 출력에 가까운 레이어(`linear.bias`)에서 Gradient가 0에 가깝게 수렴하는 것은 학습이 충분히 수렴함에 따른 자연스러운 현상이다.

### 1.4. Inception-v1

- **성능**: 75.8%의 안정적인 정확도를 보였다.
- **Epoch 단위 Gradient**: Inception-50(`Unknown-5.png`)과 유사하게 Epoch 진행에 따라 전반적인 Gradient가 완만하게 감소하며 안정적인 흐름을 보였다. 그래프에서 파란색 선(`conv2d.1.conv.bias`)과 주황색 선(`conv.1.conv.weight`, Inception 모듈 내부의 초기 Conv)은 **유사한 스케일에서 안정적으로 감소**하며, 녹색 선(`linear.bias`)은 0에 가깝지만 꾸준히 유지된다.
- **Layer 별 Gradient**: 계층 간 Gradient 불균형이 적고, 전반적으로 모든 레이어가 학습에 기여하고 있어 Inception 구조의 **내부 효율성**을 시각적으로 확인시켜 준다.

### 2. 결론

본 실험은 동일한 데이터셋(CIFAR-10)에서 네트워크 구조(VGG vs. Inception) 및 깊이(16 vs. 50 layers)의 영향을 비교 분석했다. 첨부된 Gradient Flow 그래프들은 이러한 분석을 시각적으로 강력하게 뒷받침한다.

1. **VGG 구조는 깊이 확장에 매우 취약하다.** VGG-50의 급격한 성능 하락과 그래프에서 명확히 드러나는 초기 및 깊은 레이어의 **심각한 Gradient 소실**은 **잔차 연결(Residual Connection)과 같은 Gradient 전달 메커니즘이 부재**할 때, 깊이 증가가 오히려 학습을 방해함을 명백히 보여준다. 상위 분류기 레이어만 불규칙하게 학습되는 현상이 관찰되었다.
2. **Inception 구조는 깊이 확장에도 상대적으로 안정적인 성능과 Gradient 흐름을 유지한다.** Inception-50의 Gradient Flow 그래프는 깊은 구조에서도 **모든 레이어에서 Gradient가 고르게 흐르는 모습**을 보여준다. 이는 Inception 모듈의 **구조적 다양성, 병렬 처리, 그리고 1x1 Convolution을 통한 효율적인 차원 축소** 덕분이다. Inception은 명시적인 잔차 연결 없이도 Gradient 소실을 효과적으로 완화한다.
3. 딥러닝 모델 선택 및 설계 시, 단순히 깊이를 늘리는 것을 넘어 **Gradient 흐름의 안정성과 학습 최적화 용이성을 보장하는 아키텍처**를 고려해야 한다. 특히 배포 환경에서는 **성능, 연산량뿐만 아니라 구조적 효율성**을 겸비한 Inception 계열의 이점이 더욱 부각될 수 있다.

---

### 5. 반면, ResNet은 layer을 아무리 많이 쌓더라도 성능이 하락하지 않는다고 배웠습니다. 과연 그러한지 역시 layer의 수가 18개, 50, 101개 일때의 성능 변화를 확인해 보세요.

(ResNet-18, ResNet-50, ResNet-101 이라고 검색하면 구현 코드를 찾을 수 있을 겁니다.)

| **모델** | **최종 정확도 (%)** | **파라미터 수** | **총 학습 시간 (초)** |
| --- | --- | --- | --- |
| ResNet-18 | 94.56 | 약 11M | 약 2800초 |
| ResNet-50 | 93.54 | 약 25M | 약 6400초 |
| ResNet-101 | 93.69 | 약 44M | 약 11000초 |

### 2. 분석

### 2.1. 깊이에 따른 성능 및 효율성 Trade-off

- **성능 유지의 견고함**: ResNet-18부터 ResNet-101까지 세 모델 모두 CIFAR-10에서 **93~94%의 높은 정확도를 기록**했다. 이는 네트워크 깊이가 크게 증가하더라도 **성능이 급격히 하락하지 않고 안정적으로 유지**되는 ResNet의 구조적 강인성을 명확히 보여준다.
- **효율성 Trade-off**:
    - **파라미터 수**: 모델의 깊이에 비례하여 파라미터 수는 최대 **4배 이상(ResNet-101 vs. ResNet-18) 증가**했다.
    - **학습 시간**: 깊이에 따라 학습 시간 또한 **3~4배 이상 선형적으로 증가**했다.
    - 하지만 이러한 자원 증가에도 불구하고, ResNet-50과 101은 ResNet-18보다 **소폭 낮은 결과를 보이거나 유사**했다. 이는 성능 향상 없이 연산 자원 낭비 가능성을 보여주며, **ResNet-18이 가장 효율적인 선택지**임을 시사한다.

### 2.2. Gradient 흐름 분석: Residual 구조의 효과성

- **전반적 Gradient 흐름**: 모든 ResNet 모델에서 초기 계층(`conv1` 등)에서는 비교적 높은 Gradient가 나타났고, 이후 깊은 계층으로 갈수록 Gradient가 감소하는 경향을 보인다.
- **ResNet-18**: 전체적으로 **Gradient 크기 분포가 높고**, 중간 계층에서도 Gradient가 **안정적으로 살아 있는 구조**를 보였다. 이는 학습이 효과적으로 이루어지고 있음을 의미한다.
- **ResNet-50 및 101**: 더 깊은 레이어, 특히 `layer4` 영역에서는 세 모델 모두 Gradient가 유의미하게 감소하는 경향이 관찰되었다. 이는 **skip connection이 Gradient 흐름을 돕는 강력한 메커니즘임에도 불구하고**, 네트워크가 극도로 깊어질수록 residual path를 통한 Gradient 신호의 **약화(attenuation)가 일부 존재**함을 시사한다. 즉, Residual Connection이 Vanishing Gradient를 완전히 없애는 것은 아니며, 깊이에 따른 최적화 도전은 여전히 존재한다.
- **Residual 구조의 핵심**: 이러한 Gradient 흐름 분석 결과는 **skip connection이 학습의 안정성을 유지하고 깊은 구조에서도 효과적인 학습을 가능하게 한다**는 것을 실증적으로 입증한다. VGG와 같은 모델에서 발생했던 극심한 Gradient 소실이 ResNet에서는 나타나지 않았다.

### 3. 결론

본 실험은 ResNet 계열 구조가 깊이 증가에 따른 성능 하락을 효과적으로 방지하는 **견고한 특성**을 가짐을 확인했다. 특히 **Residual Connection**은 Gradient 소실 문제를 극복하고 깊은 네트워크의 학습을 안정화하는 핵심적인 역할을 수행한다.

그러나, ResNet-50 및 101은 ResNet-18 대비 파라미터 수와 학습 시간이 크게 증가했음에도 **성능 향상이 미미**했다. 이는 실제 딥러닝 응용에서 단순히 모델을 깊게 쌓는 것보다 **정확도 유지와 연산 효율성 간의 균형**을 고려한 구조 선택이 매우 중요하며, **ResNet-18이 CIFAR-10 데이터셋에서 가장 효율적인 선택지**였음을 보여준다.